###############################################################################
# Most languages "just work" in python
###############################################################################

# foreign language text can be included into `str`s
English = 'Computer programming is the best!!!'
chinese = 'è®¡ç®—æœºç¼–ç¨‹æ˜¯æœ€å¥½çš„ï¼ï¼ï¼'
korean = 'ì»´í“¨í„° í”„ë¡œê·¸ë˜ë°ì´ ìµœê³ ì…ë‹ˆë‹¤ !!!'
vietnamese = 'láº­p trÃ¬nh mÃ¡y tÃ­nh lÃ  tá»‘t nháº¥t !!!'
arabic = '!â€Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ± Ù‡ÙŠ Ø§Ù„Ø£ÙØ¶Ù„!â€!â€'

# you can have the same string with multiple languages inside
combined = 'Computer è®¡ç®—æœº Ù‡ÙŠ Ø§Ù„Ø£ÙØ¶Ù„ tá»‘t nháº¥t !!!'

# variables can be named using any language
ì»´í“¨í„° = 'this is a variable'

# emojis work fine
emoji1 = 'ğŸ˜Š'
emoji2 = 'ğŸ˜”'
emoji3 = 'ğŸ’©'
emoji4 = 'ğŸ”«'

###############################################################################
# Some things are "just weird"
###############################################################################

# for historical reasons, some characters contain font information 
italic = 'ğ˜Šğ˜°ğ˜®ğ˜±ğ˜¶ğ˜µğ˜¦ğ˜³ ğ˜±ğ˜³ğ˜°ğ˜¨ğ˜³ğ˜¢ğ˜®ğ˜®ğ˜ªğ˜¯ğ˜¨ ğ˜ªğ˜´ ğ˜µğ˜©ğ˜¦ ğ˜£ğ˜¦ğ˜´ğ˜µ'
fraktur = 'ğ•®ğ–”ğ–’ğ–•ğ–šğ–™ğ–Šğ–— ğ–•ğ–—ğ–”ğ–Œğ–—ğ–†ğ–’ğ–’ğ–ğ–“ğ–Œ ğ–ğ–˜ ğ–™ğ–ğ–Š ğ–‡ğ–Šğ–˜ğ–™'
teletype = 'ï¼£ï½ï½ï½ï½•ï½”ï½…ï½’ã€€ï½ï½’ï½ï½‡ï½’ï½ï½ï½ï½‰ï½ï½‡ã€€ï½‰ï½“ã€€ï½”ï½ˆï½…ã€€ï½‚ï½…ï½“ï½”'
upsidedown = 'Ê‡sÇq ÇÉ¥Ê‡ sÄ± É“uÄ±É¯É¯ÉÉ¹É“oÉ¹d É¹ÇÊ‡ndÉ¯oÆ†'
circles = 'â’¸â“â“œâ“Ÿâ“¤â“£â“”â“¡ â“Ÿâ“¡â“â“–â“¡â“â“œâ“œâ“˜â“â“– â“˜â“¢ â“£â“—â“” â“‘â“”â“¢â“£'

# upper/lower case is "weird"
german = 'StrauÃŸ'

# numbers are "weird" in some languages
numbers = 'à§ªà§­' # In Tamil à§ª = 4 ; à§­ = 7

# some languages "look like each other"; this is a security issue
url1 = 'https://www.BankOfAmerica.com'
url2 = 'https://www.Î’ankÎŸfÎ‘merica.com'
url3 = 'https://www.BankOfâ€‹America.com'

greek = 'Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏ‚ÏƒÏ„Ï…Ï†Ï‡Ïˆ'

# some characters don't display
empty = 'â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹'

###############################################################################
# Code points
###############################################################################

# Unicode is the standard system for representing ALL languages on computers
# Python 3.0 was the first major programming language to have "good" Unicode support
# C/C++/Java have Unicode support; but it's not "trivial" in python

# About Unicode
# - a "grapheme" is what Unicode calls an individual unit of text being displayed;
#   it's what most people would call a "letter"/"symbol"/"character"
# - a "font" is a set of pictures associated with each grapheme
# - graphemes can be composed of one or more "characters",
#   and the same grapheme can be written in different ways

accents_1 = 'Ã¡'
accents_2 = 'aÌ'

# NOTE:
# Equality in python (and all programming languages) is not based on grapheme-equivalence;
# it is based on character-equivalence

# NOTE:
# len() function displays the number of "code points"/"characters", not the number of graphemes

# Every character has a Unicode "code point" that represents the character.
# ord() converts from `str` to the code point (`int`)
# chr() converts from the code point to `str`

# these \XXXXX represent the a with the accent
accents_1a = '\xe1'         # \x has range from 0 - 255
accents_1b = '\u00e1'       # \u has range from 0 - 65535
accents_1c = '\U000000e1'   # \U can store from 0 - 4 billion  character

# these \XXXXX represent "add an accent to the previous character"
accents_2b = 'a\u0301'      
accents_2c = 'a\U00000301'

# NOTE:
# We cannot use the `\x` notation to write the character '\u0301'.
# \x only supports code points from 0-255 (1 byte);
# \u supports code points from 0-65335 (2 bytes);
# \U supports all code points (4 bytes);


# character normalization = represent the graphemes in a standard way
# normal form composed (NFC) groups graphemes into as few characters as possible
# normal form decomposed (NFD) ungroups graphemes into as many characters as possible

import unicodedata
accents_1_NFC = unicodedata.normalize('NFC', accents_1)
accents_2_NFC = unicodedata.normalize('NFC', accents_2)

# Example: Vietnamese

vietnamese_NFD = 'laÌ£Ì‚p triÌ€nh maÌy tiÌnh laÌ€ toÌ‚Ìt nhaÌ‚Ìt !!!'
vietnamese_NFC = 'láº­p trÃ¬nh mÃ¡y tÃ­nh lÃ  tá»‘t nháº¥t !!!'

# NFKD/NFKC normal forms are used to remove "font" information
# The "K" stands for "compatibility"

# Example: Fractur
fractur_NFC = unicodedata.normalize('NFC', 'ğ•®ğ–”ğ–’ğ–•ğ–šğ–™ğ–Šğ–— ğ–•ğ–—ğ–”ğ–Œğ–—ğ–†ğ–’ğ–’ğ–ğ–“ğ–Œ ğ–ğ–˜ ğ–™ğ–ğ–Š ğ–‡ğ–Šğ–˜ğ–™')
fractur_NFD = unicodedata.normalize('NFD', 'ğ•®ğ–”ğ–’ğ–•ğ–šğ–™ğ–Šğ–— ğ–•ğ–—ğ–”ğ–Œğ–—ğ–†ğ–’ğ–’ğ–ğ–“ğ–Œ ğ–ğ–˜ ğ–™ğ–ğ–Š ğ–‡ğ–Šğ–˜ğ–™')
fractur_NFKC = unicodedata.normalize('NFKC', 'ğ•®ğ–”ğ–’ğ–•ğ–šğ–™ğ–Šğ–— ğ–•ğ–—ğ–”ğ–Œğ–—ğ–†ğ–’ğ–’ğ–ğ–“ğ–Œ ğ–ğ–˜ ğ–™ğ–ğ–Š ğ–‡ğ–Šğ–˜ğ–™')
fractur_NFKD = unicodedata.normalize('NFKD', 'ğ•®ğ–”ğ–’ğ–•ğ–šğ–™ğ–Šğ–— ğ–•ğ–—ğ–”ğ–Œğ–—ğ–†ğ–’ğ–’ğ–ğ–“ğ–Œ ğ–ğ–˜ ğ–™ğ–ğ–Š ğ–‡ğ–Šğ–˜ğ–™')

# Example: Arabic
# Arabic has lots of special characters, see: https://en.wikipedia.org/wiki/Arabic_script_in_Unicode

pbuh = 'ï·º'       # "Peace be upon him"
basmala = 'ï·½'   # "In the name of Allah"


# PRINCIPLE 1:
# You must normalize your strings if you want to compare them.
# Python will not normalize text for you automatically.
# It is traditional to use NFC,
# but it does not matter which one you choose.

################################################################################
# Sorting
################################################################################

# Recall that sorting in python is done ASCII-betically for English text
# For non-English text, we sort "Unicode"-betically, or by the Unicode code points of text

# Example: German
words = ['BÃ¤ren', 'KÃ¤fer', 'kÃ¼ssen', 'Ã„hnlich', 'Ã„pfel'];

# Example: Korean
korean_alphabet_ROK = ['ã„±', 'ã„²', 'ã„´', 'ã„·', 'ã„¸']
korean_alphabet_DPRK = ['ã„±', 'ã„´', 'ã„·', 'ã„²', 'ã„¸']

# PRINCIPLE 2:
# It is impossible to sort text "correctly" in any programming language
# unless you know "meta" information about the "locale" of the user (e.g. their country).

###############################################################################
# Encoding
###############################################################################

# How are strings physically stored on the computer?

# Background:
# - a bit is a 0 or a 1
# - one byte is 8 bits
# - a byte is the fundamental unit of information in computer science
# - 8 bits is large enough to store ASCII values (end hence English values, 
#   but nothing more)

# Example:
# A document with 1000 English characters needs 1000 bytes = 1kb

# The `bytes` type looks like strings but starts with a `b`
normal_strings = 'they just \n have quotes'
raw_strings = r'quotes that \n start with r' # ignores escape characters
example = b'this is a `byte`'

# NOTE:
# raw strings are just normal strings, and so they can be compared directly;
# byte strings (i.e. `bytes`) are an entirely different type;
# so they cannot be compared directly with strings.

# NOTE:
# The type `bytes` in python stores more than one byte!
# The `bytes` type represents a container of many bytes,
# and so is similar to a string.
# But because the `bytes` type can only store 8bits at each position,
# it cannot store non-ASCII (i.e. non-English) characters.

# Example:
# >>> korean = b'ì»´í“¨í„° í”„ë¡œê·¸ë˜ë°ì´ ìµœê³ ì…ë‹ˆë‹¤ !!!'
#  File "<stdin>", line 1
# SyntaxError: bytes can only contain ASCII literal characters.

# Every time you load a file in python,
# you are always reading bytes

# In order to store non-English text, we need an "encoding" which maps bytes to characters
# .encode() converts from `str` to `byte`
# .decode() converts from `byte` to `str`

# The most popular encoding is UTF-8
# This is a "variable-length" encoding, which means it uses a different number of bytes for each character
# UTF-8 is popular because it "agrees" with ASCII

bytes1 = 'hello world'.encode('utf-8')
bytes2 = 'hello world'.encode('utf-16')
bytes3 = 'hello world'.encode('utf-32')
bytes4 = 'hello world'.encode('iso-8859-1')
bytes5 = 'hello world'.encode('ascii')

# PRINCIPLE 3:
# Whenever you are working with bytes, you must know "meta" information about the encoding.

# Unfortunately, not all encodings support all languages/characters.
# Any encoding that begins with "utf" is safe for unicode characters.

chinese_bytes1 = 'è®¡ç®—æœºç¼–ç¨‹æ˜¯æœ€å¥½çš„ï¼ï¼ï¼'.encode('utf-8')
chinese_bytes2 = 'è®¡ç®—æœºç¼–ç¨‹æ˜¯æœ€å¥½çš„ï¼ï¼ï¼'.encode('utf-16')
chinese_bytes3 = 'è®¡ç®—æœºç¼–ç¨‹æ˜¯æœ€å¥½çš„ï¼ï¼ï¼'.encode('utf-32')
#chinese_bytes4 = 'è®¡ç®—æœºç¼–ç¨‹æ˜¯æœ€å¥½çš„ï¼ï¼ï¼'.encode('iso-8859-1')
#chinese_bytes5 = 'è®¡ç®—æœºç¼–ç¨‹æ˜¯æœ€å¥½çš„ï¼ï¼ï¼'.encode('ascii')

# The len() reports how many bytes a `byte` consumes.
len(chinese_bytes1)
len(chinese_bytes2)
len(chinese_bytes3)

# PRINCIPLE 4:
# Some encodings are better for some languages than others.

# UTF-8 prioritizes English, but it has become the default encoding for most languages.
# This makes a lot of Asians upset because they "waste" lots of disk space/bandwidth.
